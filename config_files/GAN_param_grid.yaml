program: /home/zcemg08/Scratch/Mol_gan2/GAN_experiments/train_no_reward.py
method:  grid
project: gan_molecular2
description : GAN molecular no reward

 Optimizer parameters


 bz              - batch size
 optimizer       - type of optimizer used for generator and discriminator model
 grad_correction - type of second order gradient correction method
 n_critic        - number of discriminator updates per one generator update
 lr_D            - discriminator learning rate
 lr_G            - generator learning rate
 loss            - type of GAN loss function
 reinforcment    - (1-scalar)*GAN_loss + scalar*RL_loss

 Discriminator parameters


 D(A,X) --> Dense1/2/3(AGGREGATE(A,CONCAT([CONV2(A,CONCAT[CONV1(A,X),X]),X])))

 CONV1/2    A,X ---> X_new, updates graph nodes embeddings
 AGGREGATE A,X ---> X_new, weigted sum of graph nodes represenations
 dense X ---> X_new, standard linear transformation

 h1_d - dim of graph nodes after 1st graph convolution
 h2_d - dim of graph nodes after 2nd graph convolution
 h3_d - dim of aggregated vector, which represents whole graph
 h4_d - dim of aggregated vector after first linear transformation

 nonlinearity_D  - actiavtion function off all but last layer
 Spectral_Norm_D - True/False -> applies spectral normalisation to
                   all discriminator weights

 batch_norm_D    - True/False --> applies batch norm after each layer
                   activations except the last one

 clip   - weight clipping parameter
 Lambda - coefficient for GP (gradient penalty added to discriminator loss)

 weight_init_D - discriminator weight initialised

 Generator parameters

 G  z --> (A,X)
 G(Z) --> dense_edge(dense3(dense2(dense1(z))))  , dense_nodes(dense3(dense2(dense1(z))))

 z_dim - dimention of random number input to generator

 h1_g - number of hidden nodes in the FIRST desne layer
 h_2g - number of hidden nodes in the SECOND dense layer
 h3_g - number of hidden nodes in the THIRD dense layer

 nonlinearity_G - type of activation after dense1/2/3 layers
 gumbel softmax applied after dense_edge and dense_nodes layers

 temp - gumbel softmax activation temperature

 batch_norm_G  - apply batch normalisation after dense 1/2/3 layers
 weight_init_G - generator weight initialisation


parameters:
  seed:
    value: 42
  Lambda:
    value: null
  clip_value:
    value: null
  z_dim:
    value: 32
  drop_out:
    value: 0.1
  bz:
    value: 64
  epochs:
    value: 10
  loss:
    value: 'wgan'
  h1_d:
    value: 128
  h2_d:
    value: 64
  h3_d:
    value: 128
  h4_d:
    value: 64
  nonlinearity_D:
    value: 'relu'
  Spectral_Norm_D:
    value: True
  batch_norm_D:
    value: False
  weight_init_D:
    value: 'normal'
  n_critic:
    value: 5
  h1_g:
    value: 128
  h2_g:
    value: 256
  h3_g:
    value: 512
  batch_norm_G:
    value: False
  nonlinearity_G:
    value: 'relu'
  weight_init_G:
    value: 'normal'
  temp:
    value: 0.1
  optimizer:
    value: 'RMS'
  grad_correction:
    value: null
  reinforcment:
    value: null
  betta1:
    value: null
  betta2:
    value: null
  alpha:
    value: 0.9
  weight_decay:
    value: 0
  lr_d:
    value: 0.001
  lr_g:
    value: 0.001
  test_size:
    values: [500,1500,2000,3000]
  lr_scheduler:
    value: 'const'
